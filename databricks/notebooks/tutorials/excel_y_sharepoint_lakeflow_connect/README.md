# Lakeflow Connect (SharePoint) + lectura nativa de Excel en Databricks (Spark)

Este tutorial muestra cÃ³mo leer **archivos Excel (.xlsx)** en Databricks de dos formas:

1) **Desde un Volume / almacenamiento** usando `spark.read.format("excel")`  
2) **Directo desde SharePoint** usando **Lakeflow Connect for SharePoint** (vÃ­a `databricks.connection`)

La idea es simple: **dejar de exportar Excel a CSV**, dejar de bajar archivos â€œa manoâ€, y empezar a tratarlos como un input serio para pipelines.

---

## QuÃ© vas a encontrar acÃ¡

- Un notebook listo para correr con ejemplos de:
  - âœ… `COPY INTO ... FILEFORMAT = EXCEL`
  - âœ… Auto Loader con `availableNow` (micro-batch)
  - âœ… Auto Loader en streaming continuo (si lo necesitÃ¡s)
  - âœ… (Opcional) sketch de DLT

ðŸ““ Notebook: `sharepoint_excel_lakeflow_connect.ipynb` *(o el nombre que le hayas puesto)*

---

## Requisitos

- Un workspace de Databricks (idealmente con Unity Catalog)
- Acceso para crear **Connections** (Catalog > Connections)
- Un tenant de Microsoft 365 con SharePoint
- Una App / Service Principal en Entra ID (Azure AD) para auth (recomendado para producciÃ³n)

---

## Paso 0 â€” Activar features (si aparecen como Preview)

En algunos workspaces estas features aparecen como â€œPreviewâ€ y se activan desde el panel de configuraciÃ³n/preview.

ðŸ“¸ Capturas sugeridas:
- Excel File Format Support:  
  ![Excel File Format Support](media/01_excel_file_format_support.png)

- Lakeflow Connect for SharePoint:  
  ![Lakeflow Connect for SharePoint](media/02_lakeflow_connect_sharepoint.png)

---

## Paso 1 â€” (Opcional) Identificar el Site ID de SharePoint

Dependiendo del setup, es comÃºn necesitar el identificador del sitio (Site ID).  
Una forma prÃ¡ctica es abrir el endpoint que expone el `id` y copiarlo.

ðŸ“¸ Captura sugerida:  
![SharePoint Site ID](media/05_sharepoint_site_id.png)

> Tip: guardate el Site ID porque despuÃ©s lo vas a usar en permisos o validaciones.

---

## Paso 2 â€” Crear App en Entra ID y permisos (Microsoft Graph)

Para automatizar (y no depender de un usuario), creÃ¡s una App Registration y le das permisos Graph.

Ejemplo tÃ­pico (depende del caso):
- `Sites.Read.All`
- `Files.Read.All`

ðŸ“¸ Captura sugerida:  
![Graph API Permissions](media/06_graph_api_permissions.png)

> RecomendaciÃ³n: en producciÃ³n, aplicÃ¡ el mÃ­nimo privilegio posible y manejÃ¡ secrets con un Secret Scope.

---

## Paso 3 â€” Crear la Connection en Databricks (SharePoint)

En Databricks:
`Catalog` â†’ `Connections` â†’ `Create connection` â†’ **SharePoint**

ðŸ“¸ Captura: dÃ³nde crear/gestionar conexiones  
![Catalog manage connections](media/03_catalog_manage_connections.png)

Luego completÃ¡s:
- Client ID
- Client secret
- Domain
- Tenant ID

ðŸ“¸ Captura: autenticaciÃ³n  
![Connection authentication](media/04_connection_authentication.png)

---

## Paso 4 â€” Probar lectura de Excel (dos caminos)

### A) Excel desde almacenamiento / Volume

```python
df = (spark.read
  .format("excel")
  .option("headerRows", 1)
  .load("/Volumes/<catalog>/<schema>/<volume>/demo.xlsx")
)
display(df)
